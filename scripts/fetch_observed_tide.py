# scripts/fetch_observed_tide.py
# å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿ã‚’30åˆ†ã”ã¨ã«å–å¾—

import pandas as pd
import re
import json
import requests
from datetime import datetime
from pathlib import Path

def fetch_tide_data(url='https://www1.kaiho.mlit.go.jp/TIDE/gauge/gauge.php?s=0163'):
    """æµ·ä¸Šä¿å®‰åºã®Webãƒšãƒ¼ã‚¸ã‹ã‚‰æ½®ä½ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    try:
        response = requests.get(url, timeout=30)
        response.encoding = 'utf-8'
        return response.text
    except Exception as e:
        print(f"ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def extract_observed_tide(content):
    """HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰5åˆ†ã”ã¨ã®è¦³æ¸¬æ½®ä½ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
    obs_section_match = re.search(
        r'è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿[ï¼š:]\s*[ï¼•5]åˆ†æ¯ç¬é–“å€¤.*?year\s+date\s+time\s+cm(.*?)(?=<|$)', 
        content, 
        re.DOTALL
    )
    
    if not obs_section_match:
        print("è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return []
    
    obs_data_text = obs_section_match.group(1)
    data_pattern = r'(\d{4})\s+(\d{1,2})\s+(\d{1,2})\s+(\d{1,2})\s+(\d{1,2})\s+(\d+|9999)'
    matches = re.findall(data_pattern, obs_data_text)
    
    data_rows = []
    for match in matches:
        year, month, day, hour, minute, tide_cm = match
        if tide_cm != '9999':  # æ¬ æå€¤ã‚’é™¤å¤–
            datetime_str = f"{year}-{month.zfill(2)}-{day.zfill(2)}T{hour.zfill(2)}:{minute.zfill(2)}:00"
            data_rows.append({
                'datetime': datetime_str,
                'tide': int(tide_cm),
                'type': 'observed'
            })
    
    return data_rows

def save_data(data, output_dir='data/observed'):
    """ãƒ‡ãƒ¼ã‚¿ã‚’JSONå½¢å¼ã§ä¿å­˜"""
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    # ç¾åœ¨ã®æ—¥ä»˜ã§ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
    today = datetime.now().strftime('%Y-%m-%d')
    output_file = Path(output_dir) / f'oita_observed_{today}.json'
    
    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°èª­ã¿è¾¼ã‚“ã§çµåˆ
    existing_data = []
    if output_file.exists():
        with open(output_file, 'r', encoding='utf-8') as f:
            existing_data = json.load(f)
    
    # é‡è¤‡ã‚’é™¤å»ã—ã¦ãƒãƒ¼ã‚¸
    all_data = existing_data + data
    unique_data = {item['datetime']: item for item in all_data}.values()
    sorted_data = sorted(unique_data, key=lambda x: x['datetime'])
    
    # ä¿å­˜
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(sorted_data, f, ensure_ascii=False, indent=2)
    
    # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ä½œæˆï¼ˆWebã‚¢ãƒ—ãƒªç”¨ï¼‰
    latest_file = Path(output_dir) / 'latest.json'
    # ç›´è¿‘24æ™‚é–“åˆ†ã®ã¿
    recent_data = [d for d in sorted_data if d['datetime'] >= (datetime.now().isoformat()[:10] + 'T00:00:00')]
    with open(latest_file, 'w', encoding='utf-8') as f:
        json.dump(recent_data[-288:], f, ensure_ascii=False, indent=2)  # 24æ™‚é–“Ã—12(5åˆ†é–“éš”)
    
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_file}")
    print(f"   ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(sorted_data)} ä»¶")
    print(f"   æœ€æ–°ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(recent_data[-288:])} ä»¶")
    
    return len(sorted_data)

def main():
    print("=" * 60)
    print("ğŸŒŠ å®Ÿæ¸¬æ½®ä½ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹")
    print(f"   å–å¾—æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    # ãƒ‡ãƒ¼ã‚¿å–å¾—
    content = fetch_tide_data()
    if content is None:
        print("âŒ ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return
    
    # ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
    data = extract_observed_tide(content)
    if not data:
        print("âŒ è¦³æ¸¬æ½®ä½ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return
    
    print(f"âœ… {len(data)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸ")
    
    # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    total_count = save_data(data)
    
    print("=" * 60)
    print("âœ… å‡¦ç†å®Œäº†")
    print("=" * 60)

if __name__ == "__main__":
    main()
