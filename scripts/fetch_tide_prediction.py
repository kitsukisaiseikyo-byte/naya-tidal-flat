# ============================================
# scripts/fetch_tide_prediction.py
# äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã‚’æ¯æ—¥0æ™‚ã«å–å¾—
# ============================================

import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime, timedelta
from pathlib import Path

AREA_CODE = "4402"  # å¤§åˆ†é¶´å´
BACK_PARAM = "3"
DAYS_TO_FETCH = 7
BASE_URL = "https://www1.kaiho.mlit.go.jp/TIDE/pred2/cgi-bin/TidePredCgi.cgi"

def fetch_prediction_data(target_date):
    """æŒ‡å®šæ—¥ã®æ½®ä½äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    params = {
        'area': AREA_CODE,
        'back': BACK_PARAM,
        'year': target_date.strftime('%Y'),
        'month': target_date.strftime('%m'),
        'day': target_date.strftime('%d')
    }
    
    try:
        response = requests.get(BASE_URL, params=params, timeout=10)
        response.raise_for_status()
        response.encoding = response.apparent_encoding
        soup = BeautifulSoup(response.text, 'html.parser')
        
        target_table = soup.find('table', bgcolor="#e3ffe3")
        if not target_table:
            return None
        
        rows = target_table.find_all('tr')
        hours_0_11 = [td.text.strip() for td in rows[0].find_all('td')[1:]]
        levels_0_11 = [td.text.strip() for td in rows[1].find_all('td')[1:]]
        hours_12_23 = [td.text.strip() for td in rows[2].find_all('td')[1:]]
        levels_12_23 = [td.text.strip() for td in rows[3].find_all('td')[1:]]
        
        hours = hours_0_11 + hours_12_23
        levels = levels_0_11 + levels_12_23
        
        data = []
        for j in range(24):
            time_str = f"{hours[j].zfill(2)}:00:00"
            level_cm = levels[j].replace(' ', '')
            datetime_str = f"{target_date.strftime('%Y-%m-%d')}T{time_str}"
            
            data.append({
                'datetime': datetime_str,
                'tide': int(level_cm),
                'type': 'prediction'
            })
        
        return data
        
    except Exception as e:
        print(f"âŒ {target_date.strftime('%Y-%m-%d')} ã®å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def save_prediction_data(all_data, output_dir='data/prediction'):
    """äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    # æ—¥ä»˜ã”ã¨ã®ãƒ•ã‚¡ã‚¤ãƒ«å
    today = datetime.now().strftime('%Y-%m-%d')
    output_file = Path(output_dir) / f'oita_prediction_{today}.json'
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_data, f, ensure_ascii=False, indent=2)
    
    # æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«
    latest_file = Path(output_dir) / 'latest.json'
    with open(latest_file, 'w', encoding='utf-8') as f:
        json.dump(all_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜: {output_file}")
    print(f"   ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(all_data)} ä»¶")

def main():
    print("=" * 60)
    print("ğŸ“Š 7æ—¥é–“æ½®ä½äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹")
    print(f"   å–å¾—æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    start_date = datetime.now().date()
    all_data = []
    
    for i in range(DAYS_TO_FETCH):
        target_date = start_date + timedelta(days=i)
        print(f"ğŸ“… {target_date.strftime('%Y-%m-%d')} ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
        
        data = fetch_prediction_data(target_date)
        if data:
            all_data.extend(data)
            print(f"   âœ… {len(data)} ä»¶å–å¾—")
        else:
            print(f"   âš ï¸ å–å¾—å¤±æ•—")
    
    if all_data:
        save_prediction_data(all_data)
        print("=" * 60)
        print(f"âœ… å…¨ {len(all_data)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†")
        print("=" * 60)
    else:
        print("âŒ ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")

if __name__ == "__main__":
    main()
